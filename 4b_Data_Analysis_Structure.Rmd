---
title: MADA Fall 2019 - Structure of a Data Analysis
subtitle: ""
author: Andreas Handel
institute: "University of Georgia"
date: "`r Sys.Date()`"
#bibliography: ../media/references.bib
output: 
  html_document:
    toc_depth: 3
---

```{r, echo = FALSE}
#just so I can use emoticons
#devtools::install_github("hadley/emo")
library(emo)
```

# Overview
In this unit, we'll discuss how to structure a data analysis to make it as efficient as possible.


# Learning Objectives
*    Be able to set up a structure for an efficient and reproducible analysis.

 
# Overall project structure
You want to set up your analysis in such a way that it makes sense to you and others and allows for a good and efficient workflow. The main components of your analysis will be data, code, results (tables, figures, etc.) and products (reports, interactive apps, slides, etc.). It is a good idea to have separate folders for each of those elements inside your main project folder. Your project folder could be a Github repository (a good idea) or not. Since you might not want to share your analysis publicly, using a private repository is often a good idea. Starting your project as an R project is also a good idea.

Inside your folders, you can have subfolders, e.g., separate folders for figures and tables. Or you could have subfolders for different types of analyses. There is no one correct way to set up things, but you should think of a logical and consistent structure before you start your project.


# How to structure your analysis
You will want different R scripts for the cleaning/wrangling/exploring part and the analysis part. The number of scripts depends on your project and your preference. In general, keeping things modular is useful. If you had one file that did the analysis and created a report, that would be ok for a small project. But then if you wanted to make a set of slides based on your results, you'll have to find a way to include the code in those slides. It would be easier to have code produce and save results such as figures, which can then be included in both a report/manuscript and slides.

Another consideration is computational time. For simple projects, your code will likely run fast. Once your analysis or data become large, parts of your code might run very long. You will then want to structure your scripts such that the computationally-intensive part is only run when absolutely needed. You definitely want to avoid a scenario where you have to wait minutes or hours as you play around with a figure to make it look the way you want.

As an example, and hopefully useful guide for your class project, I created [a public Github repository called dataanalyis-template](https://github.com/andreashandel/dataanalysis-template) which is meant as a template for doing a data analysis project. It has different folders for organizing your project. Various `readme` files are provided to explain what each folder should contain. The template also contains several example files to show how the whole project workflow (or any data analysis workflow for that matter) can work. This is, of course, only one way to structure things. You are welcome to figure out your own setup and structure. Overall, figure out what setup works best for you, while keeping in mind that it should be easily understandable/reproducible for a reader (or your colleague, PI,...). 


# Setting paths 
One problem that I encounter **every time** I teach a course like this is the use of paths that are specific only to the user's computer and do not work on someone else's machine. Do not set paths or load files from paths that only exist on your machine! The best way to avoid this is to always use _projects_ in R, always open a project by clicking on the `.Rproj` file, and only setting _relative_ paths. A _relative_ path is a file path that is relative to the main project directory. You can see how that's done in the examples we shared, e.g., in Brian's project and my data analysis template. A good helper for this is the [`here` package](https://github.com/r-lib/here). I don't use it, but Jenny Bryan recommends it. However you do things, make sure that for your exercises and especially final project, someone else can clone your repository (or otherwise copy your project if it's not on Github) to their computer and run everything, without having to have exactly your setup of folders.


# Further Resources
Several efforts to develop further tools to help improve reproducible research within the R system exist. The few I know about are listed below. I have not tried to use any of them, but feel free to try/use them as part of this class.

* For some more useful information on structuring your project such that it has a good, reproducible workflow, see [this collection of (incomplete) notes](https://whattheyforgot.org/) by Jenny Bryan and Jim Hester.
* [The `projects` R package](https://github.com/nikkrieger/projects) - meant to provide a framework for rather sophisticated projects.
* [R Open Science Reproducibility Discussion](https://ropensci.org/commcalls/2019-07-30/) - website with links to various resources related to reproducibility in R.
* [The `workflowr` R package](https://jdblischak.github.io/workflowr/) - meant to provide a structure for reproducible data analysis projects.