<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Andreas Handel" />


<title>MADA Course - Subset Selection and Regularization</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link rel="icon" type="image/png" href="media/MADAlogo.png" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>






<link rel="stylesheet" href="MADAstyle.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">MADA Course</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    General Information
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./Course_Syllabus.html">Syllabus</a>
    </li>
    <li>
      <a href="./Course_Schedule.html">Schedule</a>
    </li>
    <li>
      <a href="./Course_Communication.html">Communication</a>
    </li>
    <li>
      <a href="./Course_Assessments.html">Assessments</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Modules
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Course and Tools Introduction</a>
      <ul class="dropdown-menu" role="menu">
        <li>
          <a href="./Introduction_Course.html">Course Introduction</a>
        </li>
        <li>
          <a href="./Tools_RandRStudio.html">R and Rstudio</a>
        </li>
        <li>
          <a href="./Tools_Github_Introduction.html">GitHub Introduction</a>
        </li>
        <li>
          <a href="./Tools_Github_R_Workflow.html">GitHub and R Workflow</a>
        </li>
        <li>
          <a href="./Tools_Reproducibility.html">RMarkdown &amp; Co</a>
        </li>
        <li>
          <a href="./Tools_Reference_Management.html">Reference Management</a>
        </li>
        <li>
          <a href="./Assessment_Course_Tools_Introduction.html">Assessment: Course and Tools Introduction</a>
        </li>
      </ul>
    </li>
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Data Analysis Introduction</a>
      <ul class="dropdown-menu" role="menu">
        <li>
          <a href="./Data_Analysis_Motivation.html">Motivating Examples</a>
        </li>
        <li>
          <a href="./Data_Analysis_Overview.html">Data Analysis Overview</a>
        </li>
        <li>
          <a href="./Data_Analysis_Structure.html">Data Analysis Structure</a>
        </li>
        <li>
          <a href="./Assessment_Data_Analysis.html">Assessment: Data Analysis Introduction</a>
        </li>
      </ul>
    </li>
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">R Coding Basics</a>
      <ul class="dropdown-menu" role="menu">
        <li>
          <a href="./RBasics.html">R basics</a>
        </li>
        <li>
          <a href="./Assessment_Coding.html">Assessment: R Coding Basics</a>
        </li>
      </ul>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Project
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./Project_Information.html">Project information</a>
    </li>
    <li>
      <a href="./Project_Rubric.html">Project rubric</a>
    </li>
    <li>
      <a href="./Project_Review.html">Project review</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./Course_Resources.html">Course Resources</a>
    </li>
    <li>
      <a href="./General_Resources.html">General Resources</a>
    </li>
    <li>
      <a href="./Course_Glossary.html">Glossary</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">MADA Course - Subset Selection and Regularization</h1>
<h4 class="author">Andreas Handel</h4>
<h4 class="date">2021-03-25 08:29:29</h4>

</div>


<div id="overview" class="section level1">
<h1>Overview</h1>
<p>In this module, we will cover the related ideas of subset selection and regularization. We‚Äôll also briefly mention a few related approaches.</p>
</div>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Know what subset selection is and when to use it</li>
<li>Know what regularization is and when to use it</li>
<li>Be able to implement these approaches in R</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In our previous discussion of model performance, you were introduced to the problem of <strong>overfitting.</strong> Here is a quick repeat. The idea is that you generally want to make inferences or predictions <strong>not for the data you used to fit your model, but for new/different data.</strong></p>
<p>If the goal was to fit the data you have as well as possible, you could always achieve that with a complex model. If you have N observations, a polynomial model with N-1 degrees of freedom can fit your data perfectly. However, as discussed when we talked about the <strong>bias-variance trade-off</strong>, a model that gets too close to the data used for fitting will generally not perform well when applied to new data.</p>
<p>You want a model that gets the trade-off between high bias (model not flexible enough to capture all the patterns seen in the data) and high variance (model too flexible, will capture noise as well) right.</p>
<p>I showed the figure below previously. I‚Äôm repeating it here for convenience. The yellow model, in this example, has too much bias. The model is too simple to capture all the important patterns, and it <strong>underfits</strong> the data. The green model has too much variance. It goes too close to the data used for fitting and doesn‚Äôt generalize to new data, it <strong>overfits</strong> the data. In this example, the blue model has the optimal model complexity.</p>
<div class="figure" style="text-align: center">
<img src="media/islr-fig29.jpg" alt="Bias-variance tradeoff. Source: ISLR." width="70%" />
<p class="caption">
Bias-variance tradeoff. Source: ISLR.
</p>
</div>
<p>We talked about cross-validation and how it can be used to help you determine if you over-fit (or underfit). What we haven‚Äôt discussed is how to find a model that gets the trade-off between bias and variance right. Two common techniques are subset selection and regularization.</p>
<p>The general idea for these strategies is that if you let the model use all the predictors you have (this is called a <em>full</em> or <em>saturated</em> model), it might be too big and overfit the data. You can get some hint of that if the cross-validated performance measure for a full model is not better than that for a null model or a single-predictor model. The latter ones are likely too simple and underfit (unless none or a single predictor describes your data), while the full model is likely too flexible and overfits. So you want to find a model that only uses the important/useful predictors.</p>
</div>
<div id="subset-selection" class="section level1">
<h1>Subset selection</h1>
<p>You might have already seen subset selection in another statistics class. In this case, this is a refresher. Though you might not have used the approaches I describe here to perform subset selection. Subset selection is also called <em>variable selection</em> or <em>feature selection</em>. The overall idea is that you try every model with any combination of predictors, evaluate its performance (using cross-validation to get an honest estimate of model performance on new data), and pick the reduced/sub-model with the best performance. The performance here is whatever metric you decided to optimize on (e.g., SSR, RMSE, Accuracy, F1 score, etc.).</p>
<p>The approach of trying models with all combinations of predictors is called <em>exhaustive subset selection</em> or <em>best subset selection</em>. If you have only a few predictors, you can try every combination of predictors. However, once you have more than 5-10 predictors (depending on the number of observations), trying every combination of sub-models would be too time-consuming. Therefore, most of the time, such an exhaustive feature selection is not feasible, and most of the time, one of the approaches described next is used.</p>
<div id="forward-selection" class="section level2">
<h2>Forward selection</h2>
<p>In forward selection, you start with the null-model, then evaluate all 1-predictor models. The best-performing 1-predictor model is chosen, and you build all 2-predictor models on top of that chosen 1-predictor model. You choose the best 2-preditor model, then go on to add a 3rd, and do that until adding any further predictors does not lead to a model that performs better (<strong>again, measured by CV on the test set</strong>) than the smaller model. E.g., you might find that adding any of the remaining 3rd predictors does not lead to a better-performing model than the 2-predictor model you already have. Then you stop here and pick the 2-predictor model as your best.</p>
<p>Let‚Äôs consider an example. Say you want to predict <code>BMI</code> based on <code>age</code>, <code>sex</code>, <code>calorie intake</code>, and <code>exercise level</code>. Since BMI is continuous, we can consider a linear regression model, and we might try to minimize RMSE between model-predicted BMI and actual measurements. We start by computing RMSE for the null model. Then we compute RMSE for all 1-predictor models. Assume that all 1-predictor models have RMSE lower than the null model, and the lowest RMSE (all computed using cross-validation) of the 1-predictor models is the one that includes <code>calorie intake</code>. Next, you try all 2-predictor models that include <code>calorie intake</code>. Among those, a model with <code>calorie intake + exercise level</code> has the lowest RMSE, and it‚Äôs also lower than the model with <code>calorie intake</code> only. Next, you try all 3-predictor models that include the 2 chosen predictors. In this example, only 2 of those 3-predictor models are possible, namely <code>calorie intake + exercise level + age</code> and <code>calorie intake + exercise level + sex</code>. Let‚Äôs assume both of those models have RMSE that is larger than the 2 predictor model. Note, it is important to compute RMSE through cross-validation since the RMSE evaluated on the data used for fitting will always be lower for the bigger model. We thus found our best model, namely one that includes the 2 predictors <code>calorie intake + exercise level</code>.</p>
</div>
<div id="backward-selection" class="section level2">
<h2>Backward selection</h2>
<p>Backward selection is essentially the reverse to forward selection: You start with the model with all predictors, then evaluate all models with one predictor dropped. The smaller model with the best performance is your choice. You then drop each predictor at a time from that model. You continue until dropping predictors does not improve performance anymore. Then you keep the model with the best performance. Note that this method does not work on datasets that have more predictors than observations (e.g., many omics type data).</p>
<p>Let‚Äôs consider the above example again. We now start with a model that includes all 4 predictors and compute cross-validated RMSE. Next, we drop each predictor and try all 3-predictor models. Let‚Äôs say the one that dropped <code>exercise level</code> is the 3-predictor model with the lowest RMSE, lower than all other 3-predictor models, and lower than the full model. Next, we drop each variable from the current best candidate 3-predictor model, which is <code>age + sex + calorie intake</code>. All of the 2-predictor models have RMSE larger than the 3-predictor model. We thus found our final model, with the 3 predictors <code>age + sex + calorie intake</code>.</p>
</div>
<div id="other-selection-methods" class="section level2">
<h2>Other selection methods</h2>
<p>You might have noticed that in my example, forward selection and backward selection do not necessarily produce the same model. That is true in reality as well, even when applied to the same data, the two different approaches might lead to different final models. Also, as in my example, often (but not always), the final model determined through backward selection tends to include more predictors than a model determined through forward selection. In such a case, you have to decide which model to choose. Or, better yet, use one of the methods described next.</p>
<p>A problem with both forward- and backward selection is that they do not try all models. They use what is called a <em>greedy</em> strategy. Once they included or excluded a predictor, it can never be dropped or included. Once you picked your 1-predictor model, all subsequent models will consist of that 1 predictor model - even if there might be a better performing 2-predictor model that doesn‚Äôt include the 1st predictor you picked. The same holds for dropping predictors in backward selection.</p>
<p>There have recently been a number of approaches that try to be somewhat smarter in finding the best model, without having to try every combination. The <code>caret</code> package has several methods, such as Genetic Algorithms and Simulated Annealing. Those methods often do a good job finding a very good sub-model in a reasonable amount of time. While the only guarantee that you found the single best model is to try them all using exhaustive subset selection, these modern approaches usually find a model that is either the best or at least close to the best while doing so rather efficiently. See <a href="https://topepo.github.io/caret/">sections 18-22 of the caret manual</a> for more information. You don‚Äôt need to read them now, but you‚Äôll likely need to refer to them when we practice feature selection. (The <code>mlr</code> package has similar selection methods.)</p>
</div>
<div id="further-comments" class="section level2">
<h2>Further comments</h2>
<p>Most commonly, subset selection is used by starting with a full model that includes each predictor/feature in linear form only. However, there is no reason why one couldn‚Äôt build a full model that includes every predictor in more complicated forms. For instance, one could start with a model that includes each term linear and quadratic, e.g.</p>
<p><span class="math display">\[Y=b_0 + b_1X_1 + b_2X_1^2 + b_3 X_2 + b_4 X_2^2 + ...\]</span></p>
<p>One could then perform subset selection by removing each term (though if one removes the linear form, one also needs to remove higher orders). It is rarely done, and I don‚Äôt think I‚Äôve seen an example of this in the literature. If interactions or higher-order terms are suspected, one often uses a different model that performs some form of feature selection automatically. We‚Äôll visit some of those models soon.</p>
<p>You might have learned other approaches to compare model performance, e.g., using AIC or similar metrics or F-tests. As previously mentioned, I consider cross-validation the most robust and reliable method and, thus, do not discuss the others. AIC or similar might sometimes be necessary if the cross-validation approach takes too long to run, e.g., if you have a lot of data or a complicated model. I don‚Äôt see a use for any of the other methods. (But I‚Äôm happy to change my mind if someone gives me some convincing reasons üòÉ.)</p>
<p>Sometimes, you have predictors of particular interest, e.g., your main exposure. What if the subset selection algorithm removes those variables? You have to decide. Most routines allow you to specify some variables that must be kept in the model so that you can perform subset selection on the others. It is also worth thinking about what the model is trying to tell you. If it throws out the variable you are interested in, it means it might not be as important in influencing the outcome as you thought. This is a situation that requires careful thinking and judgement and then based on your best judgment, you proceed.</p>
<p>For more details on subset selection, <a href="https://statlearning.com/">see section 6.1. of ISLR</a> and <a href="https://topepo.github.io/caret/">sections 18-22 of the caret manual</a>. Those readings are optional.</p>
</div>
</div>
<div id="regularization" class="section level1">
<h1>Regularization</h1>
<p>The standard subset selection method is an ‚Äúall or none‚Äù approach. Either the predictor is included, or it is not. Another approach to making models less ‚Äúwiggly‚Äù and more ‚Äúregular‚Äù, i.e., reduce variance and thus prevent overfitting, is through an approach called <em>regularization</em>. In this approach, predictor variables and their coefficients are penalized in the cost function. It is easiest to explain with a specific example, so let‚Äôs consider a linear model. Note, however, that the regularization concept and approach is general and applies to many models beyond linear ones.</p>
<p>Our model is given by</p>
<p><span class="math display">\[Y=b_0 + b_1 X_1 + b_2 X_2 + ....\]</span>.</p>
<p>We might decide to minimize the SSR, i.e., we are minimizing a cost function</p>
<p><span class="math display">\[C = SSR=\sum_i (Y_m^i - Y_d^i)^2\]</span>.</p>
<p>Now, if we use regularization, we are going to instead minimize</p>
<p><span class="math display">\[C = SSR + f(b_j) \]</span> where the function <em>f</em> is some function of the model parameters. There are 3 main ways to choose that function <em>f</em>, described next.</p>
<div id="ridge-regression" class="section level2">
<h2>Ridge regression</h2>
<p>One way to choose the function that penalizes the predictors is to weigh each predictor by the predictor‚Äôs coefficient squared. Choosing the penalty term as the square of the coefficient is called <em>L2 regularization</em> or <em>ridge regression</em>. This leads to the cost function:</p>
<p><span class="math display">\[C = SSR + \lambda \sum_j^p b_j^2 \]</span></p>
<p>The parameter <span class="math inline">\(\lambda\)</span> decides the balance between the goodness of fit (low SSR) and the penalty for having large coefficients. Instead of trying different subsets as above and picking the best based on lowest CV performance, we now try different values of <span class="math inline">\(\lambda\)</span> and pick the model with the lowest (cross-validated) value for <em>C</em>. The parameter <span class="math inline">\(\lambda\)</span> is often referred to as the <em>tuning parameter</em>.</p>
</div>
<div id="lasso" class="section level2">
<h2>LASSO</h2>
<p>An alternative is to penalize the coefficients by their absolute value, namely using this cost function:</p>
<p><span class="math display">\[C = SSR + \lambda \sum_j^p |b_j| \]</span></p>
<p>This method is called <em>L1 regularization</em> or <em>Least Absolute Shrinkage and Selection Operator (LASSO).</em> One nice feature of LASSO (which ridge regression does not have) is that coefficients may go to 0. That means the predictor has been dropped from the model, similar to the subset selection above. One can think of the LASSO as an efficient approach for performing subset selection. It is not quite equivalent though, since, in the LASSO, the predictors that remain might have been shrunk in their impact due to the regularization penalty.</p>
</div>
<div id="elastic-net" class="section level2">
<h2>Elastic net</h2>
<p>One can also combine ridge regression and LASSO into an approach called <em>elastic net</em>, which has a cost function that is the combination of the previous two, namely:</p>
<p><span class="math display">\[ C = SSR + \lambda ( (1-\alpha) \sum_j^p b_j^2 + \alpha \sum_j^p |b_j|)\]</span></p>
<p>Now one needs to try different values for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span> to determine the model with the best (cross-validated) performance. <span class="math inline">\(\lambda\)</span> determines the overall weight given to the penalty factor, while <span class="math inline">\(\alpha\)</span> determines how the penalty should be distributed between the 2 alternative terms.</p>
</div>
<div id="further-comments-1" class="section level2">
<h2>Further comments</h2>
<p>You might have noticed that the cost function in regularization looks a bit like the equations for AIC or similar quantities. That is no accident. Both try to penalize the model for being overly complicated and thus have equations that contain terms for both model performance and model complexity while trying to find the model with the best balance.</p>
<p>If LASSO has the nice feature of potentially removing variables and thus making the model simpler, why ever use ridge regression or the elastic net? It turns out that for some problems, those other methods perform better. <a href="https://statlearning.com/">See section 6.2. of ISLR</a> for more.</p>
<p>There is a lot of math behind the regularization concept. From an applied perspective, the focus is to understand the overall idea and how to implement them. For more on regularization <a href="https://statlearning.com/">see section 6.2. of ISLR</a>, <a href="https://bradleyboehmke.github.io/HOML/regularized-regression.html">chapter 6 of HMLR</a> and <a href="https://rafalab.github.io/dsbook/">section 34.9 of IDS</a>. I won‚Äôt quiz you on those readings, but I strongly encourage you to check out the ISLR and HMLR readings and skim through them to get a better understanding of these widespread and powerful techniques.</p>
</div>
</div>
<div id="other-methods" class="section level1">
<h1>Other methods</h1>
<p>Subset selection and regularization are not the only methods that one can use to try and find the best performing model. Other methods that try to find the optimal set of predictors, often labeled <em>feature reduction methods</em>, can be used. Some of those are <em>Partial Least Squares</em>, <em>Principal Component Analysis (PCA)</em>, <em>Generalized Low Rank Models (GLRM)</em>, <em>Autoencoders</em>. Those methods are for instance discussed in <a href="https://bradleyboehmke.github.io/HOML/">chapters 17-19 of HMLR</a> and <a href="https://statlearning.com/">section 6.3. of ISLR</a>. We won‚Äôt cover those, but if you are interested, take a look.</p>
</div>
<div id="further-comments-2" class="section level1">
<h1>Further comments</h1>
<p>In addition to having potentially better performance, smaller models are also easier to interpret and potentially use. Having smaller and simpler models can sometimes be of little importance and other times of great importance.</p>
<p>For instance, if you want to build a model that allows doctors to predict the chance that a patient has a certain disease, you might want to have a model that only uses the fewest (or easiest/cheapest to measure) variables to obtain good performance. So if you collect a lot of data, some based on checking patient symptoms and some on bloodwork, you might not want to use all those variables in your model. In fact, <strong>you might not even want the best performing model</strong>. Let‚Äôs say that you had data on 10 predictors, 5 for symptom variables (e.g., body temperature and similar), and 5 variables that each come from a different lab test. You‚Äôll perform subset selection (or LASSO) and find that the best performing model retains 3 symptom variables and 2 lab tests. Let‚Äôs say its performance is 95% (I‚Äôm purposefully fuzzy about what that performance exactly is since it doesn‚Äôt matter. It could be accuracy, or F1 score, or AUC, or‚Ä¶). But you also find that another model that contains 4 symptom variables and no lab tests has 85% performance. Which do you choose? That comes back to our previous discussion about assessing model quality: Performance is one and is an important measure, but it‚Äôs not the only one. In this case, since you could get data on the 4 symptoms very quickly and cheaply, you might want to recommend that model for most doctors offices, and only use the better, but more time-consuming and expensive model with the 2 lab tests in settings such as high-risk populations in the hospital.</p>
<p>In contrast, if you are a bank that tries to predict fraud by having complicated models that constantly analyze various data streams, you might not care how complicated and big your model is, only that the performance in flagging fraudulent transactions is as good as possible.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
